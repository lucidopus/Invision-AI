# Image Inpainting with Grounding DINO, SAM, and Stable Diffusion

This project demonstrates a powerful image inpainting pipeline that utilizes **Grounding DINO**, **Segment Anything Model (SAM)**, and **Stable Diffusion** to edit images by inpainting specific objects based on text prompts. This allows for dynamic and context-aware modifications to images, such as replacing or altering specific objects within a scene.

## Table of Contents
1. [Project Overview](#project-overview)
2. [Model Components](#model-components)
   - [Grounding DINO](#grounding-dino)
   - [Segment Anything Model (SAM)](#segment-anything-model-sam)
   - [Stable Diffusion](#stable-diffusion)
3. [Installation](#installation)
4. [Example](#example)


## Project Overview

This project integrates three powerful models to provide an end-to-end image inpainting pipeline:
- **Grounding DINO**: A model for detecting and segmenting objects in images based on textual descriptions.
- **SAM (Segment Anything Model)**: A model for generating segmentation masks around detected objects.
- **Stable Diffusion**: A generative model for inpainting, used here to fill in or replace parts of the image based on a given prompt and mask.

The workflow allows you to input an image and a description of an object you want to modify. The system will then identify the object in the image, mask it, and use the Stable Diffusion model to replace or modify the object according to your prompt.

## Model Components

### Grounding DINO

[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is a vision-language model that connects visual and textual representations, enabling the detection of objects described by text. This model is used in the project to predict bounding boxes and labels for the target objects in the image.

### Segment Anything Model (SAM)

The [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) is a segmentation model that generates precise object masks. SAM takes bounding boxes or points as input and produces a mask identifying the target object in the image. In this pipeline, SAM is used to refine the segmentation and prepare the image for inpainting.

### Stable Diffusion

[Stable Diffusion](https://github.com/CompVis/stable-diffusion) is a deep learning model capable of generating high-quality images from text prompts. Here, Stable Diffusion is used to perform inpaintingâ€”replacing or modifying parts of an image based on a given prompt, while using the mask generated by SAM to ensure the changes are applied to the right regions.

## Installation

To run this project, ensure you have the following dependencies installed:

1. Python 3.8 or later
2. Required libraries:
   - torch
   - diffusers
   - huggingface_hub
   - numpy
   - Pillow
   - requests
   - dotenv
   - GroundingDINO
   - segment_anything

You can install the required libraries using pip:

```bash
pip install torch diffusers huggingface_hub numpy Pillow requests python-dotenv
```

Additionally, you will need to clone the Grounding DINO repository and download the models:
```bash
git clone https://github.com/IDEA-Research/GroundingDINO.git
```

Download Pre-trained Segment Anything Model
```bash
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
```

## Example

Input Image:
![input](https://github.com/user-attachments/assets/6299ac70-246f-4ffd-b188-df0b784b0797)

Target Object: Wall
Prompt: Change the wall theme to reflect a tribal vibe

Output Image:
<img width="605" alt="output" src="https://github.com/user-attachments/assets/16e6d184-4898-4454-94e1-663a85ad5ee3">
